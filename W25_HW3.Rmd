---
title: "W25_HW3"
author: Cameron Bentz
date: March 24, 2025
output: pdf_document
---

# Problem 1: Age Data Preprocessing

Suppose that we have age data including the following numbers in sorted order. Then answer the questions below.

```{r age-data}
age <- c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 
         25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 
         35, 36, 40, 45, 46, 52, 70)
```

## (a)  Smoothing by bin means (bin depth = 3)

```{r smoothing}
depth <- 3
nbins <- floor(length(age) / depth)


smoothed <- c() # create empty vector
for (i in 0:(nbins - 1)) { # loop through bins of data
  start <- i * depth + 1
  end <- start + depth - 1
  bin <- age[start:end] # add age data to bin
  bin_mean <- mean(bin) # calculate mean
  smoothed <- c(smoothed, rep(bin_mean, depth)) # populate bin with mean values
}

# add remaining values
if (length(age) > nbins * depth) {
  remaining <- age[(nbins * depth + 1):length(age)]
  smoothed <- c(smoothed, rep(mean(remaining), length(remaining)))
}

smoothed_result <- data.frame(Original = age, Smoothed = smoothed)
smoothed_result
```
### Comment: Smoothing by bin means reduces variance and helps reveal patterns, but can distort outliers. 


## (b) Outlier detection using IQR

``` {r IQR-outliers}
Q1 <- quantile(age, 0.25)
Q3 <- quantile(age, 0.75)
IQR_val <- IQR(age)

lower_bound <- Q1 - (1.5 * IQR_val)
upper_bound <- Q3 + (1.5 * IQR_val)

outliers <- age[age < lower_bound | age > upper_bound]
cat("The outliers for the data set are:", outliers, "\n")
```

## (c) Min-max normalization of age 35 to [0.0, 1.0]

```{r min-max}
minmax <- (35 - min(age)) / (max(age) - min(age))
cat("The min-max normalization of age 35 to [0.0, 1.0] is:", minmax, "\n")
```

## (d) Z-score normalization of age 35

```{r z-score}
mean_age <- mean(age)
sd_age <- sd(age)
z_score <- (35 - mean_age) / sd_age
cat("The z-score normalization of age 35 is:", z_score, "\n")
```

## (e) Decimal scaling normalization of age 35

```{r decimal-scaling}
k <- nchar(as.character(max(abs(age))))
decimal_scaled <- 35 / (10^k)
cat("The decimal scaling normalization of age 35 is:", decimal_scaled, "\n")
```

# Problem 2: Min-Max Normalization Function

```{r normalization}
minmax_normalize <- function(a, min_new, max_new) {
  min_old <- min(a)
  max_old <- max(a)
  scaled <- (a - min_old) / (max_old - min_old)
  scaled * (max_new - min_new) + min_new
}

# Test usage

cat("The min max normalization function with values 10, 20 produces:\n")
cat(paste(minmax_normalize(age, 10, 20), collapse = "\n"), "\n\n")
cat("The min max normalization function with values 0.0 , 1.0 produces:\n")
cat(paste(minmax_normalize(age, 0, 1), collapse = "\n"))
```

# Problem 3: Information Gain for Decision Tree

The formula for information gain is given by 
$\text{InfoGain}(T, A) = \text{Entropy}(T) - \sum_{v \in \text{Values}(A)} \frac{|T_v|}{|T|} \cdot \text{Entropy}(T_v)$,
where $T$ is the full dataset and $T_v$ is a subset corresponding to value $v$ of attribute $A$.

```{r info-gain}
data <- data.frame(
  department = c("sales", "sales", "sales", "systems", "systems", "systems", "systems",
                 "marketing", "marketing", "secretary", "secretary"),
  age = c("31_35", "26_30", "31_35", "21_25", "31_35", "26_30", 
          "41_45", "36_40", "31_35", "46_50", "26_30"),
  salary = c("46K_50K", "26K_30K", "31K_35K", "46K_50K", "66K_70K", "46K_50K", 
             "66K_70K", "46K_50K", "41K_45K", "36K_40K", "26K_30K"),
  status = c("senior", "junior", "junior", "junior", "senior", "junior", 
             "senior", "senior", "junior", "senior", "junior"),
  count = c(30, 40, 40, 20, 5, 3, 3, 10, 4, 4, 6)
)

#print(data)

entropy <- function(p) {
  p <- p[p > 0] 
  -sum(p * log2(p))
}

# Step 1: calculate overall dataset entropy
status_total <- aggregate(count ~ status, data = data, sum)
total_count <- sum(status_total$count)
p_class <- status_total$count / total_count
entropy_total <- entropy(p_class)
cat("The entropy of the dataset is:", entropy_total, "\n")

# Step 2: calculate informtion for attributes
info_gain <- function(df, attr) {
  vals <- unique(df[[attr]]) # get unique attribute values
  weighted_entropy <- 0
  
  for (val in vals) {
    subset <- df[df[[attr]] == val, ] 
    subset_total <- sum(subset$count) # count number of records in subset
    junior_count <- sum(subset$count[subset$status == "junior"]) 
    senior_count <- sum(subset$count[subset$status == "senior"])
    
    p_junior <- junior_count / subset_total #probability (junior)
    p_senior <- senior_count / subset_total #probability (senior)
    
    subset_entropy <- entropy(c(p_junior, p_senior)) 
    
    weighted_entropy <- weighted_entropy + (subset_total / total_count) * subset_entropy
  }
  
  # info gain = total entropy - weighted subset entropy
  gain <- entropy_total - weighted_entropy 
  return(gain)
}

# Calculate info gain for each attribute
cat("The info gain for department is:", info_gain(data, "department"), "\n")
cat("The info gain for age is:", info_gain(data, "age"), "\n")
cat("The info gain for salary is:", info_gain(data, "salary"), "\n")
```

# Problem 4: If-Then Rules from Decision Tree

```{r rules}
cat(" Root of decision tree is Salary (highest info gain)\n",
    "First split is on Department\n",
    "Age split is not needed\n")

cat(" If salary = 26k_30k then status = junior\n",
    "If salary = 31k_35k then status = junior\n",
    "If salary = 36k_40k then status = senior\n",
    "If salary = 41k_45k then status = junior\n",
    "If salary = 46k_50k and department = sales then status = senior\n",
    "If salary = 46k_50k and department = systems then status = junior\n",
    "If salary = 46k_50k and department = marketing then status = senior\n",
    "If salary = 66k_70k then status = senior")
```